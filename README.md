# Teacher English: Assistente de IA para Gram√°tica e Vocabul√°rio

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.9+-yellow?style=for-the-badge&logo=python" alt="Python Version">
  <img src="https://img.shields.io/badge/flask-3.1.*+-red?style=for-the-badge&logo=flask&logoColor=white"  alt="Flask Version">
  <img src="https://img.shields.io/badge/LM_Studio-Compatible-informational?style=for-the-badge&logo=ai" alt="LM Studio Compatible">
</p>

Um servi√ßo RESTful em Flask que integra um Large Language Model (LLM) local (via LM Studio) para auxiliar no ensino e aprendizado de ingl√™s. Ele oferece avalia√ß√£o gramatical, tradu√ß√£o de palavras com exemplos contextuais e a verifica√ß√£o do status do servi√ßo LLM.

---

## üéØ Objetivo do Projeto

O `Teacher English` visa **simplificar e agilizar o processo de avalia√ß√£o de gram√°tica e a consulta de vocabul√°rio para estudantes e professores de ingl√™s**. Ao integrar um LLM local, o projeto oferece uma solu√ß√£o aut√¥noma e eficiente para feedback gramatical instant√¢neo e enriquecimento de vocabul√°rio com exemplos pr√°ticos, tornando o aprendizado e o ensino mais interativos e acess√≠veis.

---

## ‚ú® Funcionalidades

O `Teacher English` exp√µe os seguintes endpoints de API:

* **Avalia√ß√£o Gramatical (`/api/grammar/evaluate`):**
    * Envie uma frase em ingl√™s via POST.
    * Receba uma an√°lise gramatical detalhada, incluindo corre√ß√µes, erros identificados e um percentual de assertividade.
* **Tradu√ß√£o e Contexto de Palavras (`/api/translate/word`):**
    * Forne√ßa uma palavra em ingl√™s via POST.
    * Obtenha sua tradu√ß√£o para o portugu√™s, tipo gramatical e frases de exemplo com tradu√ß√£o para demonstrar seu uso contextual.
* **Status do LLM (`/api/llm/status`):**
    * Verifique a disponibilidade e o status operacional do Large Language Model subjacente via GET.
    * Garanta que o servi√ßo de IA esteja ativo e operante.

---

## üöÄ Como Rodar

Siga os passos abaixo para configurar e executar o `Teacher English` em sua m√°quina.

### Pr√©-requisitos

Certifique-se de ter o seguinte instalado:

* **Python 3.9+:** Essencial para rodar a aplica√ß√£o Flask.
* **LM Studio:** Software para baixar e rodar LLMs localmente. Baixe em [lmstudio.ai](https://lmstudio.ai/).
* **Um modelo LLM compat√≠vel:** Por exemplo, `bartowski/llama-3.2-1b-instruct` ou `phi-3-mini-4k-instruct-GGUF`. Voc√™ pode baix√°-los diretamente pelo LM Studio.

### 1. Configura√ß√£o do Ambiente

1.  **Clone o Reposit√≥rio:**
    ```bash
    git clone https://github.com/EmmanoelMonteiro/teacher-english-ia.git
    cd teacher_english
    ```

2.  **Crie e Ative o Ambiente Virtual:**
    ```bash
    python -m venv .venv
    # No Windows (PowerShell):
    .venv\Scripts\Activate.ps1
    # No Windows (CMD):
    .venv\Scripts\activate.bat
    # No macOS / Linux:
    source .venv/bin/activate
    ```

3.  **Instale as Depend√™ncias:**
    ```bash
    pip install Flask python-dotenv pydantic openai
    ```
    Ou, se voc√™ j√° gerou um `requirements.txt`:
    ```bash
    pip install -r requirements.txt
    ```

4.  **Crie o Arquivo de Vari√°veis de Ambiente (`.env`):**
    Na raiz do projeto (`teacher_english/`), crie um arquivo chamado `.env` com o seguinte conte√∫do. **Ajuste o `LLM_MODEL_NAME` para o nome exato do modelo que voc√™ carregou no seu LM Studio.**

    ```dotenv
    # .env
    # Vari√°veis de Ambiente para a aplica√ß√£o teacher_english

    # --- Configura√ß√µes do LLM ---
    # URL base do servidor de infer√™ncia do LM Studio.
    # Geralmente √© localhost na porta 1234.
    LLM_BASE_URL=http://localhost:1234/v1

    # Chave de API para o LM Studio. Para uso local, qualquer string serve.
    LLM_API_KEY=lm-studio_api_key_local

    # Nome do modelo LLM carregado no LM Studio.
    # AJUSTE ESTE VALOR para o nome EXATO do modelo que voc√™ carregou.
    # Ex: gemma-2-2b-it, lmstudio-community/bartowski/llama-3.2-1b-instruct, etc.
    # Voc√™ pode encontrar este nome na aba 'Local Inference Server' do LM Studio.
    LLM_MODEL_NAME=bartowski/llama-3.2-1b-instruct 
    ```

5.  **Crie os Arquivos `__init__.py`:**
    Certifique-se de que os arquivos `__init__.py` (que podem ser vazios) existam em cada subdiret√≥rio do `src/` para que o Python reconhe√ßa a estrutura como um pacote. A estrutura deve ser a seguinte:

    ```
    teacher_english/
    ‚îú‚îÄ‚îÄ .venv/
    ‚îú‚îÄ‚îÄ src/
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py         <-- Adicione aqui
    ‚îÇ   ‚îú‚îÄ‚îÄ core/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py     <-- Adicione aqui
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ entities.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ use_cases.py
    ‚îÇ   ‚îú‚îÄ‚îÄ adapters/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py     <-- Adicione aqui
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ llm_adapter.py
    ‚îÇ   ‚îú‚îÄ‚îÄ infrastructure/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py     <-- Adicione aqui
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ llm_api_client.py
    ‚îÇ   ‚îú‚îÄ‚îÄ web/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py     <-- Adicione aqui
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ controllers.py
    ‚îÇ   ‚îî‚îÄ‚îÄ main.py
    ‚îú‚îÄ‚îÄ .env
    ‚îú‚îÄ‚îÄ requirements.txt
    ‚îî‚îÄ‚îÄ README.md
    ```
    Voc√™ pode criar esses arquivos vazios usando `touch __init__.py` (Linux/macOS) ou `type nul > __init__.py` (Windows CMD) em cada diret√≥rio correspondente.

### 2. Configura√ß√£o do LLM no LM Studio

1.  **Abra o LM Studio.**
2.  **Baixe o Modelo Desejado:** Na aba "Search", procure e baixe um modelo compat√≠vel (ex: `bartowski/llama-3.2-1b-instruct`). Salve-o em um **SSD** para melhor desempenho.
3.  **Carregue o Modelo:** Na aba "My Models", selecione o modelo baixado para carreg√°-lo na mem√≥ria.
4.  **Inicie o Servidor de Infer√™ncia Local:** V√° para a aba "Local Inference Server" e clique em "Start Server". Verifique se o servidor est√° rodando na porta `1234` (ou ajuste a vari√°vel `LLM_BASE_URL` no seu `.env` se for diferente).

### 3. Inicie a Aplica√ß√£o Flask

Com seu ambiente virtual ativado e o LM Studio rodando o servidor de infer√™ncia local, execute a aplica√ß√£o Flask a partir da **raiz do projeto**:

```bash
(.venv) python -m src.main
```

A aplica√ß√£o estar√° acess√≠vel em `http://127.0.0.1:5000/`

## üíª Endpoints da API

Todos os endpoints est√£o prefixados com `/api.`

1. Avaliar Gram√°tica
* **Endpoint:** `POST /api/grammar/evaluate`
* **Descri√ß√£o:** Avalia a gram√°tica de uma frase em ingl√™s e fornece feedback.
* **Corpo da Requisi√ß√£o (JSON):**

```JSON

{
    "phrase": "I has a new car and very happy."
}
```
* **Exemplo de Resposta (JSON):**

```JSON

{
    "comentarios": "A frase foi corrigida para garantir concord√¢ncia verbal e uso correto do adjetivo. 'Has' foi corrigido para 'have' e 'very happy' para 'very happy'.",
    "erros_encontrados": [
        "Concord√¢ncia verbal (has -> have)",
        "Uso incorreto de adv√©rbio/adjetivo (very happy -> very happy)"
    ],
    "frase_corrigida": "I have a new car and I am very happy.",
    "frase_original": "I has a new car and very happy.",
    "percentual_assertividade": 75.0
}
```

2. Traduzir Palavra e Sugerir Contexto
* **Endpoint:** `POST /api/translate/word`
* **Descri√ß√£o:** Traduz uma palavra do ingl√™s para o portugu√™s, indica seu tipo gramatical e oferece frases de exemplo com contexto.
* **Corpo da Requisi√ß√£o (JSON):**
```JSON
{
    "word": "apple"
}
```
* **Exemplo de Resposta (JSON):**
```JSON

{
    "palavra_original": "apple",
    "sugestoes_frases": [
        "She loves to eat an apple every morning. (Ela adora comer uma ma√ß√£ todas as manh√£s.)",
        "The new store sells only the best apples. (A nova loja vende apenas as melhores ma√ß√£s.)",
        "An apple a day keeps the doctor away. (Uma ma√ß√£ por dia mant√©m o m√©dico afastado.)"
    ],
    "tipo_gramatical": "substantivo",
    "traducao_pt": "ma√ß√£"
}
```

3. Status do LLM
* **Endpoint:** `GET /api/llm/status`
* **Descri√ß√£o:** Verifica o status operacional do LLM no LM Studio.
* **Exemplo de Resposta (JSON - Ativo):**
```JSON

{
    "mensagem": "LLM est√° ativo e respondendo.",
    "modelo_carregado": "gemma-2-2b-it",
    "status": "ativo",
    "tempo_resposta_ms": 65.21
}
```
* **Exemplo de Resposta (JSON - Inativo):**
```JSON

{
    "mensagem": "N√£o foi poss√≠vel conectar ao servidor do LLM. Detalhes: [Detalhes do erro de conex√£o]",
    "modelo_carregado": null,
    "status": "inativo",
    "tempo_resposta_ms": null
}
```
